---
title: "AoT_PublicData"
author: "Ana√Øs Ladoy^[anais.ladoy@epfl.ch]"
date: "04/20/2018"
output: html_document
---

###1. Download the calibrated public data
```{r, results='hide'}
#Download file

url <- "http://www.mcs.anl.gov/research/projects/waggle/downloads/datasets/AoT_Chicago.public.latest.tar.gz"
destfile <- 'Data/aot_public.tar.gz'

# Download file
file <- download.file(url,destfile = destfile,cacheOK = FALSE)

# Look at the files in the tar.gz
untar(destfile,exdir="Data")
untarFile <- untar(destfile,list=TRUE)
```

###2. Read the csv files (data + metadata)
```{r, results='hide'}
#Read files
library(data.table)

start.time <- Sys.time()
nodes <- read.csv(paste('Data/',untarFile[which(grepl('nodes',untarFile)==TRUE)],sep=''))
sensors <- read.csv(paste('Data/',untarFile[which(grepl('sensors',untarFile)==TRUE)],sep=''))
provenance <- read.csv(paste('Data/',untarFile[which(grepl('provenance',untarFile)==TRUE)],sep=''))
data <- fread(paste('Data/',untarFile[which(grepl('data',untarFile)==TRUE)],sep=''),showProgress=FALSE)
end.time <- Sys.time()
print(end.time-start.time)

#Basic informations about the csv files
str(nodes) #Structure of nodes.csv
str(sensors) #Structure of sensors.csv
str(provenance) #Structure of provenance.csv
head(data) #Head of data.csv
```

###3. Data Cleaning and Wrangling

There are still duplicated rows in the public dataset (calibrated data) :  
-*2,759* rows are duplicated
-*7,048* rows have the same combinaison of node_id/parameter/sensor/plugin but with different values recorded

Wait for Charlie's answer and see how we will handle duplicates.

```{r, results='hide'}
#Data Wrangling
library(fasttime)
library(dplyr)

#Assign data format
start.time <- Sys.time()
data$timestamp <- fastPOSIXct(data$timestamp,tz='America/Chicago') #Timestamp
data$value <- as.numeric(data$value) #Int
end.time <- Sys.time()
print(end.time-start.time)

#Save a copy of the data 
data.copy <- data

#Data Cleaning, duplicated rows
duplicates.all <-data[duplicated(data) | duplicated(data,fromLast=TRUE),] #Exactly same rows (saving the two duplicated rows)
duplicates.diffValues <- anti_join(data[duplicated(data[,c('node_id','timestamp','sensor','parameter','plugin')]) | duplicated(data[,c('node_id','timestamp','sensor','parameter','plugin')],fromLast=TRUE),], duplicates.all) #Rows with different values for the same node, parameter and sensor (saving the two duplicated rows)

data <- data[!duplicated(data, by=c('node_id','timestamp','plugin','sensor','parameter')),] #Keep only the first duplicated row (not necessarly the good one if duplicated with different values)

#Number of nodes where the same parameter is measured
NbSensorsPerNode <-aggregate(data[,c('node_id','parameter','sensor')],by=list(data$node_id,data$parameter),FUN=function(x) length(unique(x)))

```


```{r}
#Subset of dataframe with only temperature
library(ggplot2)
data.temp <- data[data$parameter=='temperature',]
data.hum <- data[data$parameter=='humidity',]

#List of the parameters measured 
unique(data$parameter) %>% print()

#List of sensors measuring temperature and humidity
sensors.temp <- unique(data.temp$sensor) %>% print() #Temperature
unique(data.hum$sensor) %>% print() #Humidity

#Descriptive statistics about the values of temperature measured by each sensor
for (i in 1:length(sensors.temp)) {
  print(paste('Descriptive statistics of temperature values measured by sensor ',sensors.temp[i]))
  print(summary(data.temp[data.temp$sensor==sensors.temp[i],]$value))
  print(paste('Number of nodes where temperature is measured with the sensor ',sensors.temp[i]))
  print(length(unique(data.temp[data.temp$sensor==sensors.temp[i],]$node_id)))
}

#Keep only the values measured by the sensor HTU21D for the temperature parameter
data.temp <- data.temp[data.temp$sensor==sensors.temp[2],]

#Descriptive statistics for the humidity measurements (Relative Humidity RH)
summary(data.hum$value)

```
**Selection of a sensor for temperature :** In 97% of the nodes, three sensors (TSYS01,HTU21D and BMP180) are measuring temperature. After comparing the distribution of the different measurements, we kept the values measured by sensor HTU21D for temperature.


###4. Reshape the dataframe and compute the daily peak values
```{r, results='hide'}
#Function to compute the maximum value recorded at a node
#The function reshapes the dataframe (timestamp in index) and compute the daily maximum for all the parameter measured.
#The missing values are omitted during the computation of mean values (na.rm=TRUE)
library(xts)
library(zoo)

daily_peak <- function(data) {
  
  var.node <- data$node_id[1]
  var.data <- data[,-1]
  
  #Reshaphe with dcast
  data <- dcast(var.data , timestamp ~ parameter + sensor, value.var = 'value')
  xts <- xts(data[,-1],data$timestamp)
  
  agg <- apply.daily(xts,FUN=function(x) apply(x,2,function(y) max(y))) %>% fortify.zoo(names = c(Index= 'timestamp'))
  return(agg)
}

```


```{r, results='hide'}

nodes.id <- unique(data$node_id) #Unique nodes id for the for loop
data.agg <- data.frame() #Empty df to store the aggregate nodes


start.time <- Sys.time()
#Compute for each node_id the daily max (only for temperature and humidity and each sensor) and bind in a dataframe
for (i in 1:length(nodes.id)) {
  daily_max <- daily_peak(data[which(data$node_id==nodes.id[i]),])
  daily_max$node_id <- nodes.id[i]
  data.agg <- bind_rows(data.agg,daily_max)
}
end.time <- Sys.time()
print(end.time-start.time)


#Change the timestamp format to keep only YYYY-MM-DD
data.agg$timestamp <- as.Date(trunc(data.agg$timestamp,units='days'))

#Compute the standard deviation for each day (proxy to assess the spatial variability as we aggregate the values of the different nodes for a given day)
data.agg.std <- aggregate(data.agg[,c('humidity_HTU21D','temperature_HTU21D')], by=list(time=data.agg$timestamp), FUN=function(x) sd(x,na.rm=TRUE))
data.agg.len <- aggregate(data.agg[,c('humidity_HTU21D','temperature_HTU21D')], by=list(time=data.agg$timestamp), FUN=function(x) length(x))
#The March, 17th seems interesting both for the temperature and humidity 

#Standard deviation of the measurements between the different nodes at 17/03/2018
print(data.agg.std[data.agg.std$time=='2018-03-17',])
data.agg.day <- data.agg[data.agg$timestamp=='2018-03-17',c('node_id','humidity_HTU21D','temperature_HTU21D')]
#Number of nodes having humidity measurements for 17/03/2018
print(data.agg.len[data.agg.len$time=='2018-03-17',]$humidity_HTU21D-nrow(data.agg.day[is.na(data.agg.day$humidity_HTU21D),]))
#Number of nodes having temperature measurements for 17/03/2018
print(data.agg.len[data.agg.len$time=='2018-03-17',]$temperature_HTU21D-nrow(data.agg.day[is.na(data.agg.day$temperature_HTU21D),]))

#Merge with the nodes metadata and transform to Spatial Dataframe
data.agg.day.spt<-merge(nodes[,c('node_id','lat','lon')],data.agg.day,by.x = 'node_id',by.y='node_id')
  
#Save in csv
write.csv(data.agg.day.spt,'Data/daily_peak_20180317.csv',row.names = FALSE)

```